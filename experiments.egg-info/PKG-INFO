Metadata-Version: 2.4
Name: experiments
Version: 0.0.1
Summary: Lightweight experiment staging primitives (artifacts, tasks, executors)
Author: Jacob Springer
License: MIT
Project-URL: Homepage, https://example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pyyaml

# Experiments Framework

A lightweight Python framework for defining and running large-scale experiments on Slurm clusters.

## What is it?

This framework lets you:
- **Define experiments as artifacts** with automatic dependency tracking
- **Run massive grid searches** with minimal boilerplate
- **Manage Slurm jobs** through a simple CLI
- **Track experiment history** and view logs easily
- **Skip completed work** automatically with smart exists checks
- **Print commands** for sequential execution or debugging

Instead of writing bash scripts and managing job dependencies manually, you define Python dataclasses that represent your experiments. The framework handles job submission, dependencies, tracking, and skipping automatically.

## Quick Start

### 1. Define Your Artifacts

```python
from dataclasses import dataclass
from experiments import Artifact, ArtifactSet, SlurmExecutor, Task, Project

@dataclass(frozen=True)
class PretrainedModel(Artifact):
    learning_rate: float
    num_epochs: int
    
    def get_requirements(self):
        return {
            'partition': 'general',
            'gpus': '4',
            'cpus': 8,
            'time': '24:00:00',
        }
    
    def construct(self, builder: Task):
        # Create config file
        builder.create_yaml_file(
            f'{builder.artifact_path}/{self.relpath}/config.yaml',
            {'lr': self.learning_rate, 'epochs': self.num_epochs}
        )
        # Run training
        builder.run_command(
            f'python train.py --config {builder.artifact_path}/{self.relpath}/config.yaml'
        )
```

### 2. Initialize Project and Create a Grid Search

```python
# Initialize your project (auto-creates ~/.experiments/projects/my-experiment/project.json)
Project.init('my-experiment')

# Create 12 models (3 learning rates × 4 epoch counts)
models = ArtifactSet.from_product(
    cls=PretrainedModel,
    params={
        'learning_rate': [1e-3, 1e-4, 1e-5],
        'num_epochs': [10, 20, 50, 100],
    }
)

# Setup executor (paths/defaults can be set in project.json)
executor = SlurmExecutor(
    setup_command='source ~/.bashrc && conda activate myenv'
)

executor.stage('pretrain', models)

if __name__ == '__main__':
    executor.auto_cli()
```

### 3. Launch and Manage

```bash
# Preview what will run
python my_experiment.py drylaunch --slurm time=12:00:00 partition=gpu

# Launch all jobs
python my_experiment.py launch --slurm time=2-00:00:00 gpus=4 cpus=8

# Check status and history
python my_experiment.py history

# View logs for a specific job
python my_experiment.py cat 12345
python my_experiment.py cat 12345_0  # Specific array task

# Cancel jobs
python my_experiment.py cancel

# Print commands for sequential execution
python my_experiment.py print > commands.sh
```

## Key Features

### Automatic Dependencies

Artifacts can depend on other artifacts. The framework automatically:
- Computes the correct execution order via topological sort
- Submits jobs in dependency-ordered tiers
- Sets up Slurm dependencies between tiers

```python
@dataclass(frozen=True)
class FinetunedModel(Artifact):
    base_model: PretrainedModel  # Dependency!
    dataset: str
    
    def construct(self, builder: Task):
        # Access base model path
        base_path = f'{builder.artifact_path}/{self.base_model.relpath}'
        builder.run_command(f'python finetune.py --base {base_path}')
```

### Smart Job Grouping

Artifacts with different resource requirements are automatically grouped into separate Slurm array jobs:

```python
# These will become 2 separate jobs:
# - Job 1: Models with 4 GPUs
# - Job 2: Models with 8 GPUs
models = [
    Model(gpus='4', ...),  # Group 1
    Model(gpus='4', ...),  # Group 1
    Model(gpus='8', ...),  # Group 2
]
```

### Automatic Skip Logic

Artifacts can define existence checks to skip already-completed work:

```python
from pathlib import Path

@dataclass(frozen=True)
class TrainModel(Artifact):
    model_name: str
    epochs: int
    
    @property
    def exists(self) -> bool:
        """Check if the trained model already exists."""
        model_path = Path(f"./models/{self.model_name}/checkpoint.pt")
        return model_path.exists()
    
    def construct(self, builder: Task):
        # Only runs if exists returns False
        builder.run_command(f'python train.py --model {self.model_name}')
```

For advanced skip logic, override `should_skip()`:

```python
@dataclass(frozen=True)
class TrainModel(Artifact):
    use_cache: bool = True
    
    def should_skip(self) -> bool:
        """Custom skip logic."""
        if not self.use_cache:
            return False  # Always run if caching disabled
        return self.exists  # Otherwise check if exists
```

### Configuration Management

First run creates `~/.experiments/config.json` and per-project configs under `~/.experiments/projects/{project}/`.

Global config example (`~/.experiments/config.json`):

```json
{
  "log_directory": "~/.experiments/logs",
  "default_partition": "general",
  "default_slurm_args": {
    "general": {"time": "2-00:00:00", "cpus": 1, "requeue": false},
    "array": {"time": "2-00:00:00", "cpus": 4, "requeue": true},
    "cpu": {"time": "1-00:00:00", "cpus": 1, "requeue": false}
  },
  "project_defaults": {
    "name": "{project_name}"
  }
}
```

Customize defaults per partition. Artifact-specific requirements override these.

Per-project config is stored at `~/.experiments/projects/{project}/project.json` and looks like:

```json
{
  "config": {
    "name": "my-experiment",
    "artifact_path": "/data/outputs/my-experiment",
    "code_path": "/home/user/code",
    "default_partition": "general",
    "default_slurm_args": {
      "general": {"time": "2-00:00:00"}
    }
  }
}
```

Project config values are available at runtime via `Project.config.<key>` and exported to each task’s environment as `EXPERIMENTS_PROJECT_CONF` (JSON). Each artifact’s parameters (including `relpath`) are exported per task as `EXPERIMENTS_EXPERIMENT_CONF`.

## CLI Commands

| Command | Description |
|---------|-------------|
| `launch [stages...]` | Submit jobs to Slurm |
| `drylaunch [stages...]` | Preview without submitting |
| `history` | View all submitted jobs |
| `cat <job_id>` | View logs for a job |
| `cancel [stages...]` | Cancel running jobs |
| `print [stages...]` | Print commands for sequential execution |

### CLI Options

**launch/drylaunch/print options:**
- `--head N` - Only run the first N artifacts
- `--tail N` - Only run the last N artifacts
- `--rerun` - Ignore exists check and rerun all artifacts
- `--reverse` - Launch stages in reverse order (respects dependencies)
- `--exclude STAGE [STAGE...]` - Exclude specific stages
- `--artifact CLASS [CLASS...]` - Only run artifacts of specific types
- `--slurm KEY=VALUE [KEY=VALUE...]` - Override Slurm args for all jobs
  - Works with any supported Slurm option, e.g. `time`, `partition`, `gpus`, `cpus`, `mem`, `qos`, `account`, `output`, `error`, etc.
  - Example: `--slurm time=12:00:00 partition=gpu gpus=A100:4 mem=64G`
- `--force-launch` (launch only) - Launch even if an artifact is already running

**Examples:**
```bash
# Launch only first 5 artifacts
python script.py launch --head 5

# Launch everything except pretrain stage
python script.py launch --exclude pretrain

# Only launch FinetunedModel artifacts
python script.py launch --artifact FinetunedModel

# Force rerun even if artifacts exist
python script.py launch --rerun

# Override Slurm resources for this run
python script.py launch --slurm time=8:00:00 partition=gpu gpus=4 cpus=8

# Launch even if previously submitted tasks are still running
python script.py launch --force-launch
```

## Advanced Usage

### Multi-Stage Experiments

```python
# Stage 1: Pretrain
pretrained = ArtifactSet.from_product(
    cls=PretrainedModel,
    params={'lr': [1e-3, 1e-4], 'epochs': [10, 20]}
)

# Stage 2: Finetune (depends on pretrained)
datasets = ['cifar10', 'cifar100']
finetuned = ArtifactSet.join_product(
    pretrained, 
    ArtifactSet(datasets)
).map(
    lambda model, dataset: FinetunedModel(base_model=model, dataset=dataset)
)

executor.stage('pretrain', pretrained)
executor.stage('finetune', finetuned)

# Launch only finetuning (pretrain must have completed)
# python script.py launch finetune
```

### ArtifactSet Operations

```python
# Cartesian product
models = ArtifactSet.from_product(
    cls=Model,
    params={'lr': [1e-3, 1e-4], 'batch_size': [32, 64]}
)

# Join product (cross product of two sets)
combined = ArtifactSet.join_product(models, datasets)

# Map over items
processed = combined.map(
    lambda model, dataset: ProcessedData(model=model, dataset=dataset)
)

# Map and flatten
expanded = models.map_flatten(
    lambda model: ArtifactSet.from_product(
        cls=Variant,
        params={'model': model, 'variant': ['a', 'b', 'c']}
    )
)

# Concatenate sets
all_models = models_a + models_b

# Map-reduce
total = artifacts.map_reduce(
    map_fn=lambda a: a.compute_metric(),
    reduce_fn=sum
)
```

### Task Building Blocks

#### File Creation

```python
def construct(self, builder: Task):
    # YAML file
    builder.create_yaml_file('config.yaml', {'lr': 0.001, 'epochs': 10})
    
    # Plain text file
    builder.create_file('script.sh', '#!/bin/bash\necho hello')
    
    # Binary file
    builder.create_file('data.bin', b'\x00\x01\x02')
```

#### Commands with Arguments

```python
builder.run_command(
    'python train.py',
    vargs=['arg1', 'arg2'],  # Positional arguments
    kwargs={
        'config': 'config.yaml',
        'output': '/data/output',
        'lr': 0.001,
    },
    vformat='{v}',  # Format for positional args
    kwformat='--{k}={v}'  # Format for keyword args
)
# Produces: python train.py arg1 arg2 --config=config.yaml --output=/data/output --lr=0.001
```

#### Google Cloud Storage

```python
# Upload file
builder.upload_to_gs('/local/path/model.pt', 'gs://bucket/models/model.pt')

# Upload directory
builder.upload_to_gs('/local/path/model', 'gs://bucket/models/', directory=True)

# Download file
builder.download_from_gs('gs://bucket/data/dataset.tar', '/local/dataset.tar')

# Download directory
builder.download_from_gs('gs://bucket/data/', '/local/data', directory=True)

# Rsync upload (with smart sync)
builder.rsync_to_gs(
    '/local/checkpoints',
    'gs://bucket/checkpoints',
    delete=False,  # Don't delete remote files not in source
    checksum=True  # Use checksum instead of mtime
)

# Rsync download
builder.rsync_from_gs(
    'gs://bucket/checkpoints',
    '/local/checkpoints',
    delete=False,
    skip_existing=True
)
```

#### Other Operations

```python
# Download from web
builder.download('https://example.com/data.tar.gz', '/local/data.tar.gz')

# Download Hugging Face model
builder.download_hf_model('bert-base-uncased', '/local/models/bert')

# Create directory
builder.ensure_directory('/path/to/dir')

# Set environment variable
builder.set_env('CUDA_VISIBLE_DEVICES', '0,1,2,3')

# Set from command output
builder.set_env('NUM_GPUS', 'nvidia-smi --list-gpus | wc -l', from_command=True)
```

### Slurm Resource Requirements

All valid Slurm options are supported:

```python
def get_requirements(self):
    return {
        # Basic
        'partition': 'gpu',
        'time': '24:00:00',
        'account': 'myaccount',
        'qos': 'high',
        
        # Resources
        'nodes': 1,
        'ntasks': 1,
        'cpus': 8,  # or 'cpus_per_task'
        'mem': '32G',
        'mem_per_cpu': '4G',
        
        # GPUs (modern format)
        'gpus': '4',  # or 'A100:4'
        
        # GPUs (older format)
        'gres': 'gpu:4',
        'constraint': 'a100',
        
        # Job control
        'requeue': True,
        'signal': 'B:USR1@60',
        
        # Logs
        'output': '/custom/path/out_%A_%a.log',
        'error': '/custom/path/err_%A_%a.log',
        'separate_error': True,
        
        # Email
        'mail_type': 'END,FAIL',
        'mail_user': 'user@example.com',
    }
```

### Setup Commands

Run initialization code before each task:

```python
executor = SlurmExecutor(
    project='my-experiment',
    artifact_path='/data/outputs',
    code_path='/code',
    setup_command='''
        source ~/miniconda3/etc/profile.d/conda.sh
        conda activate myenv
        export PYTHONPATH=$PYTHONPATH:/code
    '''
)
```

### PrintExecutor for Debugging

Use `PrintExecutor` to generate bash scripts instead of submitting to Slurm:

```python
from experiments import PrintExecutor, Project

Project.init('my-experiment')
executor = PrintExecutor(setup_command='source activate myenv')

executor.stage('train', models)
executor.execute(['train'])  # Prints bash commands to stdout
```

Or use the CLI:
```bash
python script.py print > commands.sh
bash commands.sh  # Run sequentially
```

## Installation

```bash
pip install -e .
```

## Project Structure

```
experiments/
├── artifact.py      # Artifact and ArtifactSet classes
├── executor.py      # SlurmExecutor, PrintExecutor, Task
├── cli.py           # Command-line interface
└── utils.py         # Helper functions

~/.experiments/
├── config.json                      # Global configuration (includes project_defaults)
├── logs/                            # Job log files (shared)
└── projects/
    └── my-project/
        ├── project.json             # Per-project configuration (under "config")
        ├── canceled_jobs.json       # Canceled job IDs for this project
        ├── launched_jobs.json       # Running/prevent-duplicate tracking for this project
        └── stages/
            ├── pretrain/
            │   └── jobs.json        # Submission history
            └── finetune/
                └── jobs.json
```

## Examples

See `examples/` for complete examples:

### Basic Examples
- `exists_demo.py` - Using the exists property to skip completed artifacts
- `skip_conditions_demo.py` - Advanced skip conditions and custom logic

### Advanced Examples
- `multistage_training.py` - Multi-stage ML pipeline with pretraining and finetuning
- `olmo/gridsearch.py` - Large-scale LLM training grid search

## Requirements

- Python 3.10+
- Slurm cluster
- PyYAML

Optional:
- Google Cloud SDK (for GCS features)
- Hugging Face CLI (for HF model downloads)

## Best Practices

### Use Frozen Dataclasses

Make artifacts immutable and hashable:
```python
@dataclass(frozen=True)
class MyArtifact(Artifact):
    param1: str
    param2: int
```

### Artifact Paths

Artifact paths are automatically generated from class name and parameters:
```python
model = PretrainedModel(learning_rate=0.001, epochs=10)
print(model.relpath)  # "PretrainedModel/a1b2c3d4e5"
```

The hash ensures unique paths for different parameter combinations.

### Override relpath for Custom Naming

```python
@dataclass(frozen=True)
class MyModel(Artifact):
    name: str
    version: int
    
    @property
    def relpath(self) -> str:
        return f'MyModel/{self.name}_v{self.version}'
```

### Test with drylaunch

Always verify your experiment plan before launching:
```bash
python script.py drylaunch
```

This shows:
- How many jobs will be submitted
- How tasks are grouped by resources
- Dependency relationships
- Which artifacts will be skipped

### Use Stages for Organization

Organize related artifacts into stages:
```python
executor.stage('download', download_tasks)
executor.stage('preprocess', preprocess_tasks)
executor.stage('train', train_tasks)
executor.stage('eval', eval_tasks)
```

Then launch specific stages:
```bash
python script.py launch train eval  # Only train and eval
python script.py launch --exclude download  # Everything except download
```

### Efficient Grid Searches

Use `from_product` for parameter sweeps:
```python
models = ArtifactSet.from_product(
    cls=Model,
    params={
        'lr': [1e-3, 1e-4, 1e-5],
        'batch_size': [16, 32, 64],
        'optimizer': ['adam', 'sgd'],
    }
)
# Creates 3 × 3 × 2 = 18 models
```

### Handle Large Experiments

For experiments with many artifacts, use filtering:
```bash
# Test with a small subset first
python script.py launch --head 5

# Launch in batches
python script.py launch --head 100
python script.py launch --tail 100
```

### Track Progress

Monitor your experiments:
```bash
# View all submitted jobs
python script.py history

# Check specific job logs
squeue -u $USER  # See running jobs
python script.py cat <job_id>_<array_index>

# Cancel if needed
python script.py cancel
```

### Duplicate-Run Prevention

The launcher tracks each submitted artifact by its unique `relpath` and the Slurm job it was submitted to.

- Before submitting, it checks `squeue` for running jobs and skips artifacts that already have a running instance.
- Tracking is stored per-project in `~/.experiments/projects/{project}/launched_jobs.json`.
- Use `--force-launch` to skip this check and always submit new jobs.

Note: When tasks are submitted as array jobs, the tracker records both the `job_id` and the array `index` for each artifact.

## Tips

- Jobs are tracked in `~/.experiments/projects/{project}/` for history viewing
- Log files use pattern: `{log_dir}/{job_name}_{job_id}_{array_index}.log`
- Array jobs automatically map array index to task
- Dependencies use `afterok` to ensure successful completion
- File operations use `flock` for safe concurrent access
- Skipped artifacts still participate in dependency resolution

## Troubleshooting

### Jobs not starting
- Check dependencies: `python script.py drylaunch` shows dependency graph
- Verify partitions and resources are valid for your cluster
- Check `squeue -u $USER` for job status

### Artifacts keep rerunning
- Verify `exists` property is implemented correctly
- Check file paths match between `exists` and `construct`
- Use `--rerun` flag if you want to force rerun

### Import errors in jobs
- Use `setup_command` to activate environments
- Set `PYTHONPATH` if needed
- Verify `code_path` points to your source code

### Out of memory
- Adjust `mem` or `mem_per_cpu` in `get_requirements()`
- Reduce batch sizes or model sizes
- Request nodes with more memory

## License

MIT
